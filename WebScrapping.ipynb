{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import certifi\n",
    "import urllib3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from urllib.parse import urljoin\n",
    "import random\n",
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notas \n",
    "revisar de la lista de urls las paginas que no tienen paginacion y ver cuales se puede y cuales no \n",
    "*  caso easy no se encuentra paginacion\n",
    "*  revisar url hilti, no las reconoce\n",
    "*  revisar icsa e incoresa no reconoce que tenga paginacion\n",
    "*  revisar ferrepat no reconoce ningun link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code Country</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>Information</th>\n",
       "      <th>Type Pagination</th>\n",
       "      <th>Note</th>\n",
       "      <th>Secuencia de paginacion</th>\n",
       "      <th>url final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>abc sa</td>\n",
       "      <td>SBD &amp; OTHER</td>\n",
       "      <td>scrolldown</td>\n",
       "      <td>mi</td>\n",
       "      <td>no pag</td>\n",
       "      <td>https://www.abc-sa.com.ar/catalogue/all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Black &amp; Decker</td>\n",
       "      <td>SBD</td>\n",
       "      <td>page</td>\n",
       "      <td>sec+1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://ar.blackanddecker.global/productos/her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Black &amp; Decker</td>\n",
       "      <td>SBD</td>\n",
       "      <td>page</td>\n",
       "      <td>sec+1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://ar.blackanddecker.global/productos/her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>cetrogar</td>\n",
       "      <td>SBD &amp; OTHER</td>\n",
       "      <td>page</td>\n",
       "      <td>sec+1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.cetrogar.com.ar/herramientas.html?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Dewalt</td>\n",
       "      <td>SBD</td>\n",
       "      <td>page</td>\n",
       "      <td>sec+1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://ar.dewalt.global/productos/accesorios?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Code Country    Country            Name  Information Type Pagination   Note  \\\n",
       "0          ARG  Argentina          abc sa  SBD & OTHER      scrolldown     mi   \n",
       "1          ARG  Argentina  Black & Decker          SBD            page  sec+1   \n",
       "2          ARG  Argentina  Black & Decker          SBD            page  sec+1   \n",
       "3          ARG  Argentina        cetrogar  SBD & OTHER            page  sec+1   \n",
       "4          ARG  Argentina          Dewalt          SBD            page  sec+1   \n",
       "\n",
       "  Secuencia de paginacion                                          url final  \n",
       "0                  no pag            https://www.abc-sa.com.ar/catalogue/all  \n",
       "1                       1  https://ar.blackanddecker.global/productos/her...  \n",
       "2                       1  https://ar.blackanddecker.global/productos/her...  \n",
       "3                       1  https://www.cetrogar.com.ar/herramientas.html?...  \n",
       "4                       1  https://ar.dewalt.global/productos/accesorios?...  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta=r'/home/sebastian/Documentos/programas/Webscrapping/all_links.xlsx'\n",
    "df_urls=pd.read_excel(ruta,header=0,sheet_name='urls')\n",
    "df_urls['url final']=df_urls['url final'].str.replace(\" \", \"\", regex=False)\n",
    "df_urls['url final'] = df_urls['url final'].str.strip()\n",
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificacion de si es producto\n",
    "posibles_subcadenas = [\n",
    "    # Generales en e-commerce\n",
    "    '/product/', '/products/', '/producto/', '/productos/',\n",
    "    '/item/', '/items/', '/detalle/', '/detail/',\n",
    "    '/sku/', '/articulo/', '/artículos/',\n",
    "\n",
    "    # Términos comunes en español e inglés\n",
    "    '/detalle-producto/', '/ver-producto/', '/ver_producto/',\n",
    "    '/viewproduct/', '/product-detail/', '/product_info/',\n",
    "\n",
    "    # Prefijos o patrones típicos\n",
    "    '-p-', '-prod-', '-item-', '-sku-', '-detalle-',\n",
    "\n",
    "    # Patrones Amazon\n",
    "    '/dp/', '/gp/product/',\n",
    "\n",
    "    # Patrones MercadoLibre (por país)\n",
    "    '/mla-', '/mlm-', '/mlc-', '/mlv-', '/mlu-', '/mlb-', '/mls-',  # Argentina, México, Chile, Venezuela, Uruguay, Brasil, Colombia\n",
    "\n",
    "    # Easy y Sodimac (comunes en LATAM)\n",
    "    '/producto/', '/productos/', '/sku/', '/ficha/', '/ficha-producto/',\n",
    "\n",
    "    # Stanley, DeWalt, Bosch, Makita, etc.\n",
    "    '/tools/', '/catalog/', '/categories/', '/details/', '/item-details/',\n",
    "\n",
    "    # Otros posibles patrones semánticos\n",
    "    '/shop/', '/buy/', '/comprar/', '/oferta/', '/ofertas/', '/promo/', '/promocion/',\n",
    "    \n",
    "    # Extensiones finales sospechosas\n",
    "    '.html', '.htm'\n",
    "  ]\n",
    "\n",
    "# validar_url\n",
    "session = requests.Session()\n",
    "user_agents = [\n",
    "    # Lista simple para rotar headers\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64)...',\n",
    "    # Puedes expandirla con más UA reales\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contiene_paginacion(soup):\n",
    "    '''Verifica si alguna etiqueta <a> o button contiene indicios de paginacion\n",
    "    arg :soup\n",
    "    return valor(0,1,2) respuesta\n",
    "    '''\n",
    "    # Lista extendida y homogénea (en minúsculas)\n",
    "    palabras_paginacion = [\n",
    "        'siguiente', 'sig', '›', '»', '→', 'adelante', 'próximo',\n",
    "        'next', 'forward', '>', '>>', 'seguinte'\n",
    "    ]\n",
    "\n",
    "    # Buscar todos los enlaces y botones con texto que podría ser paginación\n",
    "    elementos = soup.find_all(['a', 'button', 'li'])\n",
    "\n",
    "    for el in elementos:\n",
    "        texto = el.get_text(strip=True).lower()\n",
    "        if any(p in texto for p in palabras_paginacion):\n",
    "            print(f'contiene paginacion')\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def validar_url(url, session=session, timeout=20):\n",
    "    '''Valida cada url paginada para saber si se puede o no ingresar a esta, y ademas si esta es o no la ultima pagina, esto mediante la verificacion si tiene o no un boton que permita cambiar\n",
    "     de pag\n",
    "      arg: \n",
    "        * url: url paginada\n",
    "         *session:mantiene la sesion activa\n",
    "          *timeout: tiempo de espera\n",
    "     '''\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': random.choice(user_agents),\n",
    "            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8'\n",
    "        }\n",
    "        respuesta = session.get(url, headers=headers, verify=False, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "        if respuesta.status_code == 200:\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "            \n",
    "            if contiene_paginacion(soup):\n",
    "                return 0, respuesta\n",
    "            else:\n",
    "                return 1, respuesta\n",
    "        else:\n",
    "            return 2, None\n",
    "    except Exception as e:\n",
    "        return 3, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precompilamos patrones\n",
    "PATRONES_PRODUCTO = re.compile(r'(product|item|skcard|detail)', re.I)\n",
    "PATRONES_FILTROS = re.compile(r'(category|categories|filter|#)', re.I)\n",
    "ATRIBUTOS_PRODUCTO = ['data-product-id', 'data-sku', 'data-id', 'data-item-id']\n",
    "\n",
    "def es_link_de_producto(a_tag, posibles_subcadenas):\n",
    "    href = a_tag['href'].lower()\n",
    "    clases = a_tag.get('class', [])\n",
    "    atributos = a_tag.attrs\n",
    "\n",
    "    # Condición: contiene palabra clave\n",
    "    contiene_subcadena = any(sub in href for sub in posibles_subcadenas)\n",
    "\n",
    "    # Condición: clase relacionada a producto\n",
    "    clase_relevante = any(PATRONES_PRODUCTO.search(clase) for clase in clases)\n",
    "\n",
    "    # Condición: atributos HTML de producto\n",
    "    tiene_atributo = any(attr in atributos for attr in ATRIBUTOS_PRODUCTO)\n",
    "\n",
    "    # Condición: no es un filtro ni ancla\n",
    "    no_es_filtro = not PATRONES_FILTROS.search(href)\n",
    "\n",
    "    return (contiene_subcadena or clase_relevante or tiene_atributo) and no_es_filtro\n",
    "\n",
    "def obtener_links_web_paginada(respuesta, url, posibles_subcadenas):\n",
    "    soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if es_link_de_producto(a, posibles_subcadenas):\n",
    "            full_url = urljoin(url, a['href'])\n",
    "            links.add(full_url)\n",
    "\n",
    "    return list(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_links_url(url_base: str, sec_pag: int, posibles_subcadenas: List[str], pausa: int = 10, max_errores: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae enlaces de productos de todas las páginas asociadas a una URL base paginada.\n",
    "\n",
    "    Parámetros:\n",
    "    - url_base: string con 'num_pag' como marcador para el número de página.\n",
    "    - sec_pag: incremento por página (1, 10, etc.).\n",
    "    - posibles_subcadenas: lista de palabras clave que indican que un link es de producto.\n",
    "    - pausa: segundos de espera entre solicitudes (default: 10).\n",
    "    - max_errores: número máximo de fallos consecutivos antes de detener (default: 2).\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de URLs extraídas.\n",
    "    \"\"\"\n",
    "    \n",
    "    errores_consecutivos = 0\n",
    "    numero_pagina = 0\n",
    "    links_extraidos = []\n",
    "\n",
    "    while errores_consecutivos < max_errores:\n",
    "        # Reemplazo del marcador en la URL\n",
    "        url = url_base.replace('num_pag', str(numero_pagina)) if 'num_pag' in url_base else url_base\n",
    "\n",
    "        estado, respuesta = validar_url(url)\n",
    "        print(f'estado de la url {url} es :{estado}')\n",
    "        if estado <=1:  # Estado 0 -1significa respuesta válida\n",
    "            links_pagina = obtener_links_web_paginada(respuesta, url, posibles_subcadenas)\n",
    "            print(f\"[✅] Página {numero_pagina}: {len(links_pagina)} links encontrados en {url}\")\n",
    "\n",
    "            if not links_pagina:\n",
    "                print(f\"[🛑] Página {numero_pagina} sin contenido. Deteniendo scraping.\")\n",
    "                break\n",
    "\n",
    "            if estado == 1:\n",
    "                errores_consecutivos +=1\n",
    "                print(f'la url :{url} en su pagina {sec_pag} no tiene informacion, numero de errores {errores_consecutivos}')\n",
    "            else:\n",
    "                errores_consecutivos +=0   # Resetear contador de errores\n",
    "                numero_pagina += sec_pag\n",
    "    \n",
    "            url_new=url_base.replace('num_pag', str(numero_pagina)) if 'num_pag' in url_base else url_base\n",
    "\n",
    "           \n",
    "            if url==url_new:\n",
    "                errores_consecutivos+=1\n",
    "                print(f'la url :{url} ya se evaluo, numero de errores {errores_consecutivos}')\n",
    "            else:\n",
    "                print(f'la respuesta de :{url}  cambio')\n",
    "            \n",
    "            links_extraidos.extend(links_pagina)\n",
    "    \n",
    "    \n",
    "        else:\n",
    "            errores_consecutivos += 1\n",
    "            print(f' estado para {url} es :{estado}')\n",
    "            print(f\"[⚠️] Error en {url}. Intento {errores_consecutivos} de {max_errores}\")\n",
    "\n",
    "        time.sleep(pausa)\n",
    "\n",
    "    return list(set(links_extraidos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Code Country', 'Country', 'Name', 'Information', 'Type Pagination',\n",
       "       'Note', 'Secuencia de paginacion', 'url final'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_urls.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construir_dataframe_links(df_urls: pd.DataFrame, url_base: str, lista_links: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye un DataFrame con metadatos asociados a una URL base y una lista de links de productos.\n",
    "\n",
    "    Args:\n",
    "        df_urls (pd.DataFrame): DataFrame que contiene las URLs base y sus metadatos.\n",
    "        url_base (str): URL base usada para extraer productos.\n",
    "        lista_links (List[str]): Lista de URLs de productos extraídos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con cada link asociado a su contexto (país, nombre, paginación, etc).\n",
    "    \"\"\"\n",
    "    # Validar que la URL base esté presente\n",
    "    coincidencias = df_urls[df_urls['url final'] == url_base]\n",
    "    \n",
    "    if coincidencias.empty:\n",
    "        print(f\"[⚠️] construir_dataframe_links: No se encontró metadata para URL base: {url_base}\")\n",
    "        return pd.DataFrame()  # Devuelve DataFrame vacío si no hay coincidencias\n",
    "\n",
    "    fila = coincidencias.iloc[0]\n",
    "\n",
    "    # Crear el DataFrame final replicando los metadatos por cada link\n",
    "    df_resultado = pd.DataFrame({\n",
    "        'url final': lista_links,\n",
    "        'Code Country': fila['Code Country'],\n",
    "        'Country': fila['Country'],\n",
    "        'Name': fila['Name'],\n",
    "        'Information': fila['Information'],\n",
    "        'Type Pagination': fila['Type Pagination'],\n",
    "        'Note': fila['Note'],\n",
    "        'Secuencia de paginacion': fila['Secuencia de paginacion']\n",
    "    })\n",
    "\n",
    "    print(f\"construir_dataframe_links: Se creó el DataFrame con {len(lista_links)} links para {fila['Country']} - {fila['Name']}\")\n",
    "    return df_resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_fila(df_urls: pd.DataFrame, fila: pd.Series) -> pd.DataFrame:\n",
    "    url_base = fila['url final']\n",
    "    try:\n",
    "        sec_pag = int(fila['Secuencia de paginacion'])\n",
    "    except ValueError:\n",
    "        print(f\"[⚠️] Error convirtiendo secuencia de paginación a int: {fila['Secuencia de paginacion']}\")\n",
    "        sec_pag = 1  # Valor por defecto\n",
    "\n",
    "    links = extraer_links_url(\n",
    "        url_base=url_base,\n",
    "        sec_pag=sec_pag,\n",
    "        posibles_subcadenas=posibles_subcadenas,\n",
    "        pausa=5  # Puedes ajustar esto\n",
    "    )\n",
    "\n",
    "    if links:\n",
    "        return construir_dataframe_links(df_urls, url_base, links)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Devuelve vacío si no se extrajo nada\n",
    "    \n",
    "    \n",
    "def extraer_links_todas_las_urls(df_urls: pd.DataFrame, max_workers: int = 50) -> pd.DataFrame:\n",
    "    resultados = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futuros = [\n",
    "            executor.submit(procesar_fila, df_urls, fila)\n",
    "            for _, fila in df_urls.iterrows()\n",
    "        ]\n",
    "\n",
    "        for futuro in tqdm(as_completed(futuros), total=len(futuros), desc=\"Procesando URLs\"):\n",
    "            resultado = futuro.result()\n",
    "            if not resultado.empty:\n",
    "                resultados.append(resultado)\n",
    "\n",
    "    if resultados:\n",
    "        df_consolidado = pd.concat(resultados, ignore_index=True)\n",
    "        print(f\"[✅] Se extrajeron {len(df_consolidado)} links en total.\")\n",
    "        return df_consolidado\n",
    "    else:\n",
    "        print(\"[⚠️] No se extrajo ningún link.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code Country</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>Information</th>\n",
       "      <th>Type Pagination</th>\n",
       "      <th>Note</th>\n",
       "      <th>Secuencia de paginacion</th>\n",
       "      <th>url final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>PER</td>\n",
       "      <td>Peru</td>\n",
       "      <td>Incoresa</td>\n",
       "      <td>SBD &amp; OTHER</td>\n",
       "      <td>tiene paginaciion pero no cambia la url por pag</td>\n",
       "      <td>no pag</td>\n",
       "      <td>no pag</td>\n",
       "      <td>https://incoresa.com.pe/tienda/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Code Country Country      Name  Information  \\\n",
       "1404          PER    Peru  Incoresa  SBD & OTHER   \n",
       "\n",
       "                                      Type Pagination    Note  \\\n",
       "1404  tiene paginaciion pero no cambia la url por pag  no pag   \n",
       "\n",
       "     Secuencia de paginacion                        url final  \n",
       "1404                  no pag  https://incoresa.com.pe/tienda/  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prueba=df_urls[df_urls['url final']=='https://incoresa.com.pe/tienda/']\n",
    "#df_prueba=df_urls[2:4]\n",
    "df_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[⚠️] Error convirtiendo secuencia de paginación a int: no pag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando URLs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado de la url https://incoresa.com.pe/tienda/ es :1\n",
      "[✅] Página 0: 31 links encontrados en https://incoresa.com.pe/tienda/\n",
      "la url :https://incoresa.com.pe/tienda/ en su pagina 1 no tiene informacion, numero de errores 1\n",
      "la url :https://incoresa.com.pe/tienda/ ya se evaluo, numero de errores 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando URLs: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construir_dataframe_links: Se creó el DataFrame con 31 links para Peru - Incoresa\n",
      "[✅] Se extrajeron 31 links en total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_links_webpaginadas=extraer_links_todas_las_urls(df_prueba, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url final</th>\n",
       "      <th>Code Country</th>\n",
       "      <th>Country</th>\n",
       "      <th>Name</th>\n",
       "      <th>Information</th>\n",
       "      <th>Type Pagination</th>\n",
       "      <th>Note</th>\n",
       "      <th>Secuencia de paginacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.troybilt.com/en_US/three-stage-sno...</td>\n",
       "      <td>GLOBAL</td>\n",
       "      <td>Global</td>\n",
       "      <td>Troy-Bilt</td>\n",
       "      <td>SBD</td>\n",
       "      <td>load more</td>\n",
       "      <td>no pag</td>\n",
       "      <td>no pag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url final Code Country Country  \\\n",
       "6  https://www.troybilt.com/en_US/three-stage-sno...       GLOBAL  Global   \n",
       "\n",
       "        Name Information Type Pagination    Note Secuencia de paginacion  \n",
       "6  Troy-Bilt         SBD       load more  no pag                  no pag  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='https://www.troybilt.com/en_US/three-stage-snow-blowers/vortex-2610-snow-blower/31AH5DP7B66.html?fitsOnModel=false'\n",
    "df_links_webpaginadas[df_links_webpaginadas['url final']==x]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
