{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         # Manipulaci√≥n y an√°lisis de datos en estructuras tipo DataFrame\n",
    "import requests             # Realizar solicitudes HTTP para obtener contenido web\n",
    "from bs4 import BeautifulSoup  # Parsear y extraer informaci√≥n de documentos HTML y XML\n",
    "import time                 # Controlar pausas y medir tiempos de ejecuci√≥n\n",
    "import certifi              # Proporciona certificados ra√≠z actualizados para conexiones seguras\n",
    "import urllib3              # Manejo avanzado de conexiones HTTP, incluyendo soporte para HTTPS\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # Ejecuci√≥n concurrente de tareas (paralelismo)\n",
    "from tqdm import tqdm       # Mostrar barras de progreso en bucles\n",
    "from urllib.parse import urljoin  # Construir URLs absolutas a partir de relativas\n",
    "import random               # Generar n√∫meros aleatorios, √∫til para rotar user-agents\n",
    "from typing import List     # Tipado est√°tico para listas y otras estructuras\n",
    "import re                   # Manejo y b√∫squeda de patrones con expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notas \n",
    "revisar de la lista de urls las paginas que no tienen paginacion y ver cuales se puede y cuales no \n",
    "*  caso easy no se encuentra paginacion\n",
    "*  revisar url hilti, no las reconoce\n",
    "*  revisar icsa e incoresa no reconoce que tenga paginacion\n",
    "*  revisar ferrepat no reconoce ningun link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta=r'/home/sebastian/Documentos/programas/Webscrapping/all_links.xlsx'\n",
    "df_urls=pd.read_excel(ruta,header=0,sheet_name='urls')\n",
    "df_urls['url final']=df_urls['url final'].str.replace(\" \", \"\", regex=False)\n",
    "df_urls['url final'] = df_urls['url final'].str.strip()\n",
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificacion de si es producto\n",
    "posibles_subcadenas = [\n",
    "    # Generales en e-commerce\n",
    "    '/product/', '/products/', '/producto/', '/productos/',\n",
    "    '/item/', '/items/', '/detalle/', '/detail/',\n",
    "    '/sku/', '/articulo/', '/art√≠culos/',\n",
    "\n",
    "    # T√©rminos comunes en espa√±ol e ingl√©s\n",
    "    '/detalle-producto/', '/ver-producto/', '/ver_producto/',\n",
    "    '/viewproduct/', '/product-detail/', '/product_info/',\n",
    "\n",
    "    # Prefijos o patrones t√≠picos\n",
    "    '-p-', '-prod-', '-item-', '-sku-', '-detalle-',\n",
    "\n",
    "    # Patrones Amazon\n",
    "    '/dp/', '/gp/product/',\n",
    "\n",
    "    # Patrones MercadoLibre (por pa√≠s)\n",
    "    '/mla-', '/mlm-', '/mlc-', '/mlv-', '/mlu-', '/mlb-', '/mls-',  # Argentina, M√©xico, Chile, Venezuela, Uruguay, Brasil, Colombia\n",
    "\n",
    "    # Easy y Sodimac (comunes en LATAM)\n",
    "    '/producto/', '/productos/', '/sku/', '/ficha/', '/ficha-producto/',\n",
    "\n",
    "    # Stanley, DeWalt, Bosch, Makita, etc.\n",
    "    '/tools/', '/catalog/', '/categories/', '/details/', '/item-details/',\n",
    "\n",
    "    # Otros posibles patrones sem√°nticos\n",
    "    '/shop/', '/buy/', '/comprar/', '/oferta/', '/ofertas/', '/promo/', '/promocion/',\n",
    "    \n",
    "    # Extensiones finales sospechosas\n",
    "    '.html', '.htm'\n",
    "  ]\n",
    "\n",
    "# validar_url\n",
    "session = requests.Session()\n",
    "user_agents = [\n",
    "    # Lista simple para rotar headers\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64)...',\n",
    "    # Puedes expandirla con m√°s UA reales\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contiene_paginacion(soup):\n",
    "    '''Verifica si alguna etiqueta <a> o button contiene indicios de paginacion\n",
    "    arg :soup\n",
    "    return valor(0,1,2) respuesta\n",
    "    '''\n",
    "    # Lista extendida y homog√©nea (en min√∫sculas)\n",
    "    palabras_paginacion = [\n",
    "        'siguiente', 'sig', '‚Ä∫', '¬ª', '‚Üí', 'adelante', 'pr√≥ximo','paginacion',\n",
    "        'next', 'forward', '>', '>>', 'seguinte','pagination','page','pag'\n",
    "    ]\n",
    "\n",
    "    # Buscar todos los enlaces y botones con texto que podr√≠a ser paginaci√≥n\n",
    "    elementos = soup.find_all(['a', 'button', 'li'])\n",
    "\n",
    "    for el in elementos:\n",
    "        texto = el.get_text(strip=True).lower()\n",
    "        if any(p in texto for p in palabras_paginacion):\n",
    "            print(f'contiene paginacion')\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def validar_url(url, session=session, timeout=20):\n",
    "    '''Valida cada url paginada para saber si se puede o no ingresar a esta, y ademas si esta es o no la ultima pagina, esto mediante la verificacion si tiene o no un boton que permita cambiar\n",
    "     de pag\n",
    "      arg: \n",
    "        * url: url paginada\n",
    "         *session:mantiene la sesion activa\n",
    "          *timeout: tiempo de espera\n",
    "     '''\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': random.choice(user_agents),\n",
    "            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8'\n",
    "        }\n",
    "        respuesta = session.get(url, headers=headers, verify=False, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "        if respuesta.status_code == 200:\n",
    "            soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "            \n",
    "            if contiene_paginacion(soup):\n",
    "                return 0, respuesta\n",
    "            else:\n",
    "                return 1, respuesta\n",
    "        else:\n",
    "            return 2, None\n",
    "    except Exception as e:\n",
    "        return 3, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precompilamos patrones\n",
    "PATRONES_PRODUCTO = re.compile(r'(product|item|skcard|detail)', re.I)\n",
    "PATRONES_FILTROS = re.compile(r'(category|categories|filter|#)', re.I)\n",
    "ATRIBUTOS_PRODUCTO = ['data-product-id', 'data-sku', 'data-id', 'data-item-id']\n",
    "\n",
    "def es_link_de_producto(a_tag, posibles_subcadenas):\n",
    "    href = a_tag['href'].lower()\n",
    "    clases = a_tag.get('class', [])\n",
    "    atributos = a_tag.attrs\n",
    "\n",
    "    # Condici√≥n: contiene palabra clave\n",
    "    contiene_subcadena = any(sub in href for sub in posibles_subcadenas)\n",
    "\n",
    "    # Condici√≥n: clase relacionada a producto\n",
    "    clase_relevante = any(PATRONES_PRODUCTO.search(clase) for clase in clases)\n",
    "\n",
    "    # Condici√≥n: atributos HTML de producto\n",
    "    tiene_atributo = any(attr in atributos for attr in ATRIBUTOS_PRODUCTO)\n",
    "\n",
    "    # Condici√≥n: no es un filtro ni ancla\n",
    "    no_es_filtro = not PATRONES_FILTROS.search(href)\n",
    "\n",
    "    return (contiene_subcadena or clase_relevante or tiene_atributo) and no_es_filtro\n",
    "\n",
    "def obtener_links_web_paginada(respuesta, url, posibles_subcadenas):\n",
    "    soup = BeautifulSoup(respuesta.text, 'html.parser')\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if es_link_de_producto(a, posibles_subcadenas):\n",
    "            full_url = urljoin(url, a['href'])\n",
    "            links.add(full_url)\n",
    "\n",
    "    return list(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_links_url(url_base: str, sec_pag: int, posibles_subcadenas: List[str], pausa: int = 10, max_errores: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrae enlaces de productos de todas las p√°ginas asociadas a una URL base paginada.\n",
    "\n",
    "    Par√°metros:\n",
    "    - url_base: string con 'num_pag' como marcador para el n√∫mero de p√°gina.\n",
    "    - sec_pag: incremento por p√°gina (1, 10, etc.).\n",
    "    - posibles_subcadenas: lista de palabras clave que indican que un link es de producto.\n",
    "    - pausa: segundos de espera entre solicitudes (default: 10).\n",
    "    - max_errores: n√∫mero m√°ximo de fallos consecutivos antes de detener (default: 2).\n",
    "\n",
    "    Retorna:\n",
    "    - Lista de URLs extra√≠das.\n",
    "    \"\"\"\n",
    "    \n",
    "    errores_consecutivos = 0\n",
    "    numero_pagina = 0\n",
    "    links_extraidos = []\n",
    "\n",
    "    while errores_consecutivos < max_errores:\n",
    "        # Reemplazo del marcador en la URL\n",
    "        url = url_base.replace('num_pag', str(numero_pagina)) if 'num_pag' in url_base else url_base\n",
    "\n",
    "        estado, respuesta = validar_url(url)\n",
    "        print(f'estado de la url {url} es :{estado}')\n",
    "        if estado <=1:  # Estado 0 -1significa respuesta v√°lida\n",
    "            links_pagina = obtener_links_web_paginada(respuesta, url, posibles_subcadenas)\n",
    "            print(f\"[‚úÖ] P√°gina {numero_pagina}: {len(links_pagina)} links encontrados en {url}\")\n",
    "\n",
    "            if not links_pagina:\n",
    "                print(f\"[üõë] P√°gina {numero_pagina} sin contenido. Deteniendo scraping.\")\n",
    "                break\n",
    "\n",
    "            if estado == 1:\n",
    "                errores_consecutivos +=1\n",
    "                print(f'la url :{url} en su pagina {sec_pag} no tiene informacion, numero de errores {errores_consecutivos}')\n",
    "            else:\n",
    "                errores_consecutivos +=0   # Resetear contador de errores\n",
    "                numero_pagina += sec_pag\n",
    "    \n",
    "            url_new=url_base.replace('num_pag', str(numero_pagina)) if 'num_pag' in url_base else url_base\n",
    "\n",
    "           \n",
    "            if url==url_new:\n",
    "                errores_consecutivos+=1\n",
    "                print(f'la url :{url} ya se evaluo, numero de errores {errores_consecutivos}')\n",
    "            else:\n",
    "                print(f'la respuesta de :{url}  cambio')\n",
    "            \n",
    "            links_extraidos.extend(links_pagina)\n",
    "    \n",
    "    \n",
    "        else:\n",
    "            errores_consecutivos += 1\n",
    "            print(f' estado para {url} es :{estado}')\n",
    "            print(f\"[‚ö†Ô∏è] Error en {url}. Intento {errores_consecutivos} de {max_errores}\")\n",
    "\n",
    "        time.sleep(pausa)\n",
    "\n",
    "    return list(set(links_extraidos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construir_dataframe_links(df_urls: pd.DataFrame, url_base: str, lista_links: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye un DataFrame con metadatos asociados a una URL base y una lista de links de productos.\n",
    "\n",
    "    Args:\n",
    "        df_urls (pd.DataFrame): DataFrame que contiene las URLs base y sus metadatos.\n",
    "        url_base (str): URL base usada para extraer productos.\n",
    "        lista_links (List[str]): Lista de URLs de productos extra√≠dos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con cada link asociado a su contexto (pa√≠s, nombre, paginaci√≥n, etc).\n",
    "    \"\"\"\n",
    "    # Validar que la URL base est√© presente\n",
    "    coincidencias = df_urls[df_urls['url final'] == url_base]\n",
    "    \n",
    "    if coincidencias.empty:\n",
    "        print(f\"[‚ö†Ô∏è] construir_dataframe_links: No se encontr√≥ metadata para URL base: {url_base}\")\n",
    "        return pd.DataFrame()  # Devuelve DataFrame vac√≠o si no hay coincidencias\n",
    "\n",
    "    fila = coincidencias.iloc[0]\n",
    "\n",
    "    # Crear el DataFrame final replicando los metadatos por cada link\n",
    "    df_resultado = pd.DataFrame({\n",
    "        'url final': lista_links,\n",
    "        'Code Country': fila['Code Country'],\n",
    "        'Country': fila['Country'],\n",
    "        'Name': fila['Name'],\n",
    "        'Information': fila['Information'],\n",
    "        'Type Pagination': fila['Type Pagination'],\n",
    "        'Note': fila['Note'],\n",
    "        'Secuencia de paginacion': fila['Secuencia de paginacion']\n",
    "    })\n",
    "\n",
    "    print(f\"construir_dataframe_links: Se cre√≥ el DataFrame con {len(lista_links)} links para {fila['Country']} - {fila['Name']}\")\n",
    "    return df_resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_fila(df_urls: pd.DataFrame, fila: pd.Series) -> pd.DataFrame:\n",
    "    url_base = fila['url final']\n",
    "    try:\n",
    "        sec_pag = int(fila['Secuencia de paginacion'])\n",
    "    except ValueError:\n",
    "        print(f\"[‚ö†Ô∏è] Error convirtiendo secuencia de paginaci√≥n a int: {fila['Secuencia de paginacion']}\")\n",
    "        sec_pag = 1  # Valor por defecto\n",
    "\n",
    "    links = extraer_links_url(\n",
    "        url_base=url_base,\n",
    "        sec_pag=sec_pag,\n",
    "        posibles_subcadenas=posibles_subcadenas,\n",
    "        pausa=5  # Puedes ajustar esto\n",
    "    )\n",
    "\n",
    "    if links:\n",
    "        return construir_dataframe_links(df_urls, url_base, links)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Devuelve vac√≠o si no se extrajo nada\n",
    "    \n",
    "    \n",
    "def extraer_links_todas_las_urls(df_urls: pd.DataFrame, max_workers: int = 50) -> pd.DataFrame:\n",
    "    resultados = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futuros = [\n",
    "            executor.submit(procesar_fila, df_urls, fila)\n",
    "            for _, fila in df_urls.iterrows()\n",
    "        ]\n",
    "\n",
    "        for futuro in tqdm(as_completed(futuros), total=len(futuros), desc=\"Procesando URLs\"):\n",
    "            resultado = futuro.result()\n",
    "            if not resultado.empty:\n",
    "                resultados.append(resultado)\n",
    "\n",
    "    if resultados:\n",
    "        df_consolidado = pd.concat(resultados, ignore_index=True)\n",
    "        print(f\"[‚úÖ] Se extrajeron {len(df_consolidado)} links en total.\")\n",
    "        return df_consolidado\n",
    "    else:\n",
    "        print(\"[‚ö†Ô∏è] No se extrajo ning√∫n link.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba=df_urls[df_urls['url final']=='https://incoresa.com.pe/tienda/']\n",
    "#df_prueba=df_urls[2:4]\n",
    "df_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_webpaginadas=extraer_links_todas_las_urls(df_prueba, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='https://www.troybilt.com/en_US/three-stage-snow-blowers/vortex-2610-snow-blower/31AH5DP7B66.html?fitsOnModel=false'\n",
    "df_links_webpaginadas[df_links_webpaginadas['url final']==x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscraper/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îú‚îÄ‚îÄ config.py                # Configuraci√≥n global, listas de patrones, user_agents, etc.\n",
    "‚îú‚îÄ‚îÄ utils.py                 # Funciones utilitarias generales (por ejemplo, helpers de texto, validaciones simples)\n",
    "‚îú‚îÄ‚îÄ network.py               # Funciones relacionadas a requests, sesiones, validaci√≥n de URLs\n",
    "‚îú‚îÄ‚îÄ parsing.py               # Funciones para parsear HTML, extraer links, identificar productos\n",
    "‚îú‚îÄ‚îÄ extraction.py            # Funciones para scraping y extracci√≥n de links de productos\n",
    "‚îú‚îÄ‚îÄ dataframe_builders.py    # Funciones para construir y manipular DataFrames\n",
    "‚îî‚îÄ‚îÄ main.py                  # Script principal o punto de entrada (si lo necesitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seleniumwebdriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseleniumwebdriver\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetectar_endpoint_ajax_scroll\u001b[39m(url, tiempo_espera\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seleniumwebdriver'"
     ]
    }
   ],
   "source": [
    "import seleniumwebdriver\n",
    "import time\n",
    "\n",
    "def detectar_endpoint_ajax_scroll(url, tiempo_espera=10):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Limpiar tr√°fico previo\n",
    "    driver.scopes = ['.*']  # Captura todo\n",
    "    driver.requests.clear()\n",
    "\n",
    "    # Simula varios scrolls\n",
    "    for i in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Revisar requests\n",
    "    posibles_endpoints = []\n",
    "    for request in driver.requests:\n",
    "        if request.response:\n",
    "            if \"xhr\" in request.response.headers.get('content-type', '') or \"/ajax\" in request.path or \"/load\" in request.path:\n",
    "                posibles_endpoints.append((request.method, request.url))\n",
    "\n",
    "    driver.quit()\n",
    "    return posibles_endpoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ferrepat.com/catalogo-de-productos-dewalt\"\n",
    "endpoints_detectados = detectar_endpoint_ajax_scroll(url)\n",
    "\n",
    "if endpoints_detectados:\n",
    "    print(\"üì° Endpoints detectados para scroll:\")\n",
    "    for metodo, endpoint in endpoints_detectados:\n",
    "        print(f\"{metodo} -> {endpoint}\")\n",
    "else:\n",
    "    print(\"‚ùå No se detectaron endpoints XHR asociados al scroll.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
